{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:49:16.668404Z",
     "iopub.status.busy": "2025-11-18T18:49:16.667785Z",
     "iopub.status.idle": "2025-11-18T18:49:32.864058Z",
     "shell.execute_reply": "2025-11-18T18:49:32.863222Z",
     "shell.execute_reply.started": "2025-11-18T18:49:16.668377Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import mlflow\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow.set_experiment(\"DEPI_Graduation_Project\")\n",
    "# mlflow.set_tracking_uri(\"http://127.0.0.1:5000/\")\n",
    "mlflow.set_tracking_uri(\"https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"mostafaehabakl/DEPI_Graduation_Project\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Initialized MLflow to track repo \u001b[32m\"mostafaehabakl/DEPI_Graduation_Project\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository mostafaehabakl/DEPI_Graduation_Project initialized!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Repository mostafaehabakl/DEPI_Graduation_Project initialized!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## DagsHub Integration (setup)\n",
    "\n",
    "import dagshub\n",
    "dagshub.init(repo_owner='mostafaehabakl', repo_name='DEPI_Graduation_Project', mlflow=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfversion = '2.18.0'\n",
    "sysversion = '3.11.13'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:54:51.020638Z",
     "iopub.status.busy": "2025-11-18T18:54:51.020317Z",
     "iopub.status.idle": "2025-11-18T18:54:57.279278Z",
     "shell.execute_reply": "2025-11-18T18:54:57.278658Z",
     "shell.execute_reply.started": "2025-11-18T18:54:51.020618Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# LSTM model information\n",
    "\n",
    "lstm_model = load_model(\"models/lstm_model.keras\")\n",
    "\n",
    "\n",
    "lstm_params = {\n",
    "    # Data settings\n",
    "    \"scaler\": \"MinMaxScaler\",\n",
    "    \"sequence_length\": 30,\n",
    "    \"train_split_ratio\": 0.8,\n",
    "\n",
    "    # Model architecture\n",
    "    \"model_type\": \"LSTM\",\n",
    "    \"lstm_units\": 64,\n",
    "    \"return_sequences\": False,\n",
    "    \"dense_units\": 1,\n",
    "    \"input_shape\": (30, 1),\n",
    "\n",
    "    # Training settings\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "loss_lstm = 0.015108378604054451"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:54:57.280098Z",
     "iopub.status.busy": "2025-11-18T18:54:57.279918Z",
     "iopub.status.idle": "2025-11-18T18:55:00.826012Z",
     "shell.execute_reply": "2025-11-18T18:55:00.825410Z",
     "shell.execute_reply.started": "2025-11-18T18:54:57.280084Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BiLSTM model information\n",
    "\n",
    "bilstm_model = load_model(\"models/bilstm_model.keras\")\n",
    "\n",
    "bilstm_params = {\n",
    "    # Data settings\n",
    "    \"scaler\": \"MinMaxScaler\",\n",
    "    \"sequence_length\": 30,\n",
    "    \"train_split_ratio\": 0.8,\n",
    "\n",
    "    # Model architecture\n",
    "    \"model_type\": \"BiLSTM\",\n",
    "    \"bilstm_units\": 64,\n",
    "    \"return_sequences\": False,\n",
    "    \"dense_units\": 1,\n",
    "    \"input_shape\": (30, 1),\n",
    "\n",
    "    # Training settings\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "loss_bilstm = 0.017909351736307144"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:55:00.826929Z",
     "iopub.status.busy": "2025-11-18T18:55:00.826700Z",
     "iopub.status.idle": "2025-11-18T18:55:03.613143Z",
     "shell.execute_reply": "2025-11-18T18:55:03.612523Z",
     "shell.execute_reply.started": "2025-11-18T18:55:00.826902Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# GRU model information\n",
    "\n",
    "gru_model = load_model(\"models/gru_model.keras\")\n",
    "\n",
    "gru_params = {\n",
    "    # Data settings\n",
    "    \"scaler\": \"MinMaxScaler\",\n",
    "    \"sequence_length\": 30,\n",
    "    \"train_split_ratio\": 0.8,\n",
    "\n",
    "    # Model architecture\n",
    "    \"model_type\": \"GRU\",\n",
    "    \"gru_units\": 64,\n",
    "    \"return_sequences\": False,\n",
    "    \"dense_units\": 1,\n",
    "    \"input_shape\": (30, 1),\n",
    "\n",
    "    # Training settings\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "loss_gru = 0.01584339328110218\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:55:03.614063Z",
     "iopub.status.busy": "2025-11-18T18:55:03.613848Z",
     "iopub.status.idle": "2025-11-18T18:55:06.997185Z",
     "shell.execute_reply": "2025-11-18T18:55:06.996420Z",
     "shell.execute_reply.started": "2025-11-18T18:55:03.614035Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# BiGRU model information\n",
    "\n",
    "bigru_model = load_model(\"models/bigru_model.keras\")\n",
    "\n",
    "bigru_params = {\n",
    "    # Data settings\n",
    "    \"scaler\": \"MinMaxScaler\",\n",
    "    \"sequence_length\": 30,\n",
    "    \"train_split_ratio\": 0.8,\n",
    "\n",
    "    # Model architecture\n",
    "    \"model_type\": \"BiGRU\",\n",
    "    \"bigru_units\": 64,\n",
    "    \"return_sequences\": False,\n",
    "    \"dense_units\": 1,\n",
    "    \"input_shape\": (30, 1),\n",
    "\n",
    "    # Training settings\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "loss_bigru = 0.02011227235198021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-18T18:55:06.998364Z",
     "iopub.status.busy": "2025-11-18T18:55:06.998075Z",
     "iopub.status.idle": "2025-11-18T18:55:11.215371Z",
     "shell.execute_reply": "2025-11-18T18:55:11.214820Z",
     "shell.execute_reply.started": "2025-11-18T18:55:06.998340Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# CNN model information\n",
    "\n",
    "cnn_model = load_model(\"models/cnn_model.keras\")\n",
    "\n",
    "cnn_params = {\n",
    "    # Data settings\n",
    "    \"scaler\": \"MinMaxScaler\",\n",
    "    \"sequence_length\": 30,\n",
    "    \"train_split_ratio\": 0.8,\n",
    "\n",
    "    # Model architecture\n",
    "    \"model_type\": \"1D-CNN\",\n",
    "    \"conv_filters\": 64,\n",
    "    \"conv_kernel_size\": 3,\n",
    "    \"conv_activation\": \"relu\",\n",
    "    \"input_shape\": (30, 1),\n",
    "    \n",
    "    \"flatten_layer\": True,\n",
    "\n",
    "    # Dense layers\n",
    "    \"dense_units_1\": 32,\n",
    "    \"dense_activation_1\": \"relu\",\n",
    "    \"output_units\": 1,\n",
    "    \"output_activation\": None,\n",
    "\n",
    "    # Training settings\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "}\n",
    "\n",
    "loss_cnn = 0.013367284089326859"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_prophet = {\n",
    "    \"MAE\": 14519.225429,\n",
    "    \"RMSE\": 17405.670745,\n",
    "    \"R2\": -0.254418,\n",
    "    \"Accuracy\": 0.898449,\n",
    "    \"SMAPE\": 10.362368\n",
    "}\n",
    "\n",
    "metrics_lstm = {\n",
    "    \"MAE\": 12947.080625,\n",
    "    \"RMSE\": 15202.949782,\n",
    "    \"R2\": -0.061268,\n",
    "    \"Accuracy\": 0.908004,\n",
    "    \"SMAPE\": 9.350971,\n",
    "    \"train_loss\":loss_lstm\n",
    "}\n",
    "\n",
    "metrics_bilstm = {\n",
    "    \"MAE\": 13199.284375,\n",
    "    \"RMSE\": 15277.745324,\n",
    "    \"R2\": -0.071736,\n",
    "    \"Accuracy\": 0.906212,\n",
    "    \"SMAPE\": 9.525390,\n",
    "    \"train_loss\":loss_bilstm\n",
    "}\n",
    "\n",
    "metrics_gru = {\n",
    "    \"MAE\": 13499.039062,\n",
    "    \"RMSE\": 15963.771655,\n",
    "    \"R2\": -0.170147,\n",
    "    \"Accuracy\": 0.904082,\n",
    "    \"SMAPE\": 9.760334,\n",
    "    \"train_loss\":loss_gru\n",
    "}\n",
    "\n",
    "metrics_bigru = {\n",
    "    \"MAE\": 12494.993750,\n",
    "    \"RMSE\": 14489.507686,\n",
    "    \"R2\": 0.036001,\n",
    "    \"Accuracy\": 0.911216,\n",
    "    \"SMAPE\": 9.013514,\n",
    "    \"train_loss\":loss_bigru\n",
    "}\n",
    "\n",
    "metrics_cnn = {\n",
    "    \"MAE\": 14819.969688,\n",
    "    \"RMSE\": 18178.123593,\n",
    "    \"R2\": -0.517286,\n",
    "    \"Accuracy\": 0.894696,\n",
    "    \"SMAPE\": 10.568062,\n",
    "    \"train_loss\":loss_cnn\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run LSTM_64_units at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0/runs/d3bc915c681c45918c9803e10ffcad1b\n",
      "üß™ View experiment at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# --- LSTM Run ---\n",
    "\n",
    "with mlflow.start_run(run_name=\"LSTM_64_units\"):\n",
    "    mlflow.log_params(lstm_params)  # your LSTM params dictionary\n",
    "    mlflow.log_metrics(metrics_lstm)\n",
    "    mlflow.log_artifact(\"models/lstm_model.keras\", artifact_path=\"LSTM_model\")\n",
    "    mlflow.log_artifacts(\"models/model_info \", artifact_path=\"LSTM_model/model_info\")\n",
    "    # mlflow.keras.log_model(lstm_model, \"LSTM_model\")  # will not work with dagsHub\n",
    "    mlflow.set_tag(\"tensorflow_version\", tfversion)\n",
    "    mlflow.set_tag(\"python_version\", sysversion)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run BiLSTM_64_units at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0/runs/46c03018736746ac94b229ca9f1dab5a\n",
      "üß™ View experiment at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# --- BiLSTM Run ---\n",
    "\n",
    "with mlflow.start_run(run_name=\"BiLSTM_64_units\"):\n",
    "    mlflow.log_params(bilstm_params)  # your BiLSTM params dictionary\n",
    "    mlflow.log_metrics(metrics_bilstm)\n",
    "    mlflow.log_artifact(\"models/bilstm_model.keras\", artifact_path=\"BiLSTM_model\")\n",
    "    mlflow.log_artifacts(\"models/model_info\", artifact_path=\"BiLSTM_model/model_info\")\n",
    "    # mlflow.keras.log_model(bilstm_model, \"BiLSTM_model\")  # will not work with dagsHub\n",
    "    mlflow.set_tag(\"tensorflow_version\", tfversion)\n",
    "    mlflow.set_tag(\"python_version\", sysversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run GRU_64_units at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0/runs/4c918a284b784464819ffd98a2142b54\n",
      "üß™ View experiment at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# --- GRU Run ---\n",
    "\n",
    "with mlflow.start_run(run_name=\"GRU_64_units\"):\n",
    "    mlflow.log_params(gru_params)  # your GRU params dictionary\n",
    "    mlflow.log_metrics(metrics_gru)\n",
    "    mlflow.log_artifact(\"models/gru_model.keras\", artifact_path=\"GRU_model\")\n",
    "    mlflow.log_artifacts(\"models/model_info\", artifact_path=\"GRU_model/model_info\")\n",
    "    # mlflow.keras.log_model(gru_model, \"GRU_model\")  # will not work with dagsHub\n",
    "    mlflow.set_tag(\"tensorflow_version\", tfversion)\n",
    "    mlflow.set_tag(\"python_version\", sysversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run BiGRU_64_units at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0/runs/abce1878eb574a12835405630e58db7c\n",
      "üß™ View experiment at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# --- BiGRU Run ---\n",
    "\n",
    "with mlflow.start_run(run_name=\"BiGRU_64_units\"):\n",
    "    mlflow.log_params(bigru_params)  # your BiGRU params dictionary\n",
    "    mlflow.log_metrics(metrics_bigru)\n",
    "    mlflow.log_artifact(\"models/bigru_model.keras\", artifact_path=\"BiGRU_model\")\n",
    "    mlflow.log_artifacts(\"models/model_info\", artifact_path=\"BiGRU_model/model_info\")\n",
    "    # mlflow.keras.log_model(bigru_model, \"BiGRU_model\")  # will not work with dagsHub\n",
    "    mlflow.set_tag(\"tensorflow_version\", tfversion)\n",
    "    mlflow.set_tag(\"python_version\", sysversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run CNN_64_filters at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0/runs/ca704cad8ce3463db8b7bc9ee5bd93db\n",
      "üß™ View experiment at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# --- CNN Run ---\n",
    "\n",
    "with mlflow.start_run(run_name=\"CNN_64_filters\"):\n",
    "    mlflow.log_params(cnn_params)  # your CNN params dictionary\n",
    "    mlflow.log_metrics(metrics_cnn)\n",
    "    mlflow.log_artifact(\"models/cnn_model.keras\", artifact_path=\"CNN_model\")\n",
    "    mlflow.log_artifacts(\"models/model_info\", artifact_path=\"CNN_model/model_info\")\n",
    "    # mlflow.keras.log_model(cnn_model, \"CNN_model\")  # will not work with dagsHub\n",
    "    mlflow.set_tag(\"tensorflow_version\", tfversion)\n",
    "    mlflow.set_tag(\"python_version\", sysversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN + BiGRU model information\n",
    "\n",
    "cnn_bigru_model = load_model(\"models/cnn_bigru_model.keras\")\n",
    "\n",
    "params_cnn_bigru = {\n",
    "    \"model_type\": \"CNN + BiGRU\",\n",
    "    \"conv_filters\": 64,\n",
    "    \"conv_kernel_size\": 3,\n",
    "    \"conv_activation\": \"relu\",\n",
    "    \"pool_size\": 2,\n",
    "    \"bigru_units\": 64,\n",
    "    \"dense_units\": 32,\n",
    "    \"dense_activation\": \"relu\",\n",
    "    \"output_units\": 1,\n",
    "    \"optimizer\": \"adam\",\n",
    "    \"loss\": \"mse\",\n",
    "    \"seq_length\": 30,\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 32,\n",
    "    \"validation_split\": 0.1\n",
    "}\n",
    "\n",
    "metrics_cnn_bigru = {\n",
    "    \"MAE\": 13047.209375,\n",
    "    \"RMSE\": 15231.445075,\n",
    "    \"R2\": -0.065250,\n",
    "    \"Accuracy\": 0.907292,\n",
    "    \"SMAPE\": 9.412405\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run CNN_BiGRU_64_filters at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0/runs/818297c5bb784d47adc6affb72e5e5be\n",
      "üß™ View experiment at: https://dagshub.com/mostafaehabakl/DEPI_Graduation_Project.mlflow/#/experiments/0\n"
     ]
    }
   ],
   "source": [
    "# --- CNN + BiGRU Run ---\n",
    "\n",
    "with mlflow.start_run(run_name=\"CNN_BiGRU_64_filters\"):\n",
    "    mlflow.log_params(params_cnn_bigru)  # your CNN + BiGRU params dictionary\n",
    "    mlflow.log_metrics(metrics_cnn_bigru)\n",
    "    mlflow.log_artifact(\"models/cnn_bigru_model.keras\", artifact_path=\"CNN_BiGRU_model\")\n",
    "    mlflow.log_artifacts(\"models/model_info\", artifact_path=\"CNN_BiGRU_model/model_info\")\n",
    "    # mlflow.keras.log_model(cnn_bigru_model, \"CNN_bigru_model\")  # will not work with dagsHub\n",
    "    mlflow.set_tag(\"tensorflow_version\", tfversion)\n",
    "    mlflow.set_tag(\"python_version\", sysversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Register the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Registered model 'DEPI_CNN+BiGRU_Model' already exists. Creating a new version of this model...\n"
     ]
    },
    {
     "ename": "RestException",
     "evalue": "INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRestException\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m model_run_id = \u001b[33m\"\u001b[39m\u001b[33m818297c5bb784d47adc6affb72e5e5be\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m model_uri = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mruns:/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m result = \u001b[43mmlflow\u001b[49m\u001b[43m.\u001b[49m\u001b[43mregister_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\tracking\\_model_registry\\fluent.py:138\u001b[39m, in \u001b[36mregister_model\u001b[39m\u001b[34m(model_uri, name, await_registration_for, tags, env_pack)\u001b[39m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mregister_model\u001b[39m(\n\u001b[32m     69\u001b[39m     model_uri,\n\u001b[32m     70\u001b[39m     name,\n\u001b[32m   (...)\u001b[39m\u001b[32m     74\u001b[39m     env_pack: EnvPackType | EnvPackConfig | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     75\u001b[39m ) -> ModelVersion:\n\u001b[32m     76\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Create a new model version in model registry for the model files specified by ``model_uri``.\u001b[39;00m\n\u001b[32m     77\u001b[39m \n\u001b[32m     78\u001b[39m \u001b[33;03m    Note that this method assumes the model registry backend URI is the same as that of the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m \u001b[33;03m        Version: 1\u001b[39;00m\n\u001b[32m    137\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_register_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_uri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mname\u001b[49m\u001b[43m=\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m=\u001b[49m\u001b[43mawait_registration_for\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43menv_pack\u001b[49m\u001b[43m=\u001b[49m\u001b[43menv_pack\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\tracking\\_model_registry\\fluent.py:187\u001b[39m, in \u001b[36m_register_model\u001b[39m\u001b[34m(model_uri, name, await_registration_for, tags, local_model_path, env_pack)\u001b[39m\n\u001b[32m    183\u001b[39m \u001b[38;5;66;03m# Otherwise check if there's a logged model with\u001b[39;00m\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# name artifact_path and source_run_id run_id\u001b[39;00m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    186\u001b[39m     run = client.get_run(run_id)\n\u001b[32m--> \u001b[39m\u001b[32m187\u001b[39m     logged_models = \u001b[43m_get_logged_models_from_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43martifact_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logged_models:\n\u001b[32m    189\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m MlflowException(\n\u001b[32m    190\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnable to find a logged_model with artifact_path \u001b[39m\u001b[38;5;132;01m{\u001b[39;00martifact_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33munder run \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    192\u001b[39m             error_code=ErrorCode.Name(NOT_FOUND),\n\u001b[32m    193\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\tracking\\_model_registry\\fluent.py:311\u001b[39m, in \u001b[36m_get_logged_models_from_run\u001b[39m\u001b[34m(source_run, model_name)\u001b[39m\n\u001b[32m    308\u001b[39m page_token = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m     logged_models_page = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_logged_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43msource_run\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexperiment_id\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# TODO: Filter by 'source_run_id' once Databricks backend supports it\u001b[39;49;00m\n\u001b[32m    314\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfilter_string\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mname = \u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mmodel_name\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m'\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpage_token\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpage_token\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     logged_models.extend(\n\u001b[32m    318\u001b[39m         m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m logged_models_page \u001b[38;5;28;01mif\u001b[39;00m m.source_run_id == source_run.info.run_id\n\u001b[32m    319\u001b[39m     )\n\u001b[32m    320\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m logged_models_page.token:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\tracking\\client.py:5713\u001b[39m, in \u001b[36mMlflowClient.search_logged_models\u001b[39m\u001b[34m(self, experiment_ids, filter_string, datasets, max_results, order_by, page_token)\u001b[39m\n\u001b[32m   5645\u001b[39m \u001b[38;5;129m@experimental\u001b[39m(version=\u001b[33m\"\u001b[39m\u001b[33m3.0.0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   5646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch_logged_models\u001b[39m(\n\u001b[32m   5647\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5653\u001b[39m     page_token: \u001b[38;5;28mstr\u001b[39m | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5654\u001b[39m ) -> PagedList[LoggedModel]:\n\u001b[32m   5655\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5656\u001b[39m \u001b[33;03m    Search for logged models that match the specified search criteria.\u001b[39;00m\n\u001b[32m   5657\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5711\u001b[39m \u001b[33;03m        :py:class:`LoggedModel <mlflow.entities.LoggedModel>` objects.\u001b[39;00m\n\u001b[32m   5712\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5713\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tracking_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_logged_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5714\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexperiment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_token\u001b[49m\n\u001b[32m   5715\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\tracking\\_tracking_service\\client.py:928\u001b[39m, in \u001b[36mTrackingServiceClient.search_logged_models\u001b[39m\u001b[34m(self, experiment_ids, filter_string, datasets, max_results, order_by, page_token)\u001b[39m\n\u001b[32m    922\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(experiment_ids, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\n\u001b[32m    923\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(eid, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m eid \u001b[38;5;129;01min\u001b[39;00m experiment_ids\n\u001b[32m    924\u001b[39m ):\n\u001b[32m    925\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MlflowException.invalid_parameter_value(\n\u001b[32m    926\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexperiment_ids must be a list of strings, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(experiment_ids)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m    927\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch_logged_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexperiment_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilter_string\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_results\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder_by\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpage_token\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:1053\u001b[39m, in \u001b[36mRestStore.search_logged_models\u001b[39m\u001b[34m(self, experiment_ids, filter_string, datasets, max_results, order_by, page_token)\u001b[39m\n\u001b[32m    998\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    999\u001b[39m \u001b[33;03mSearch for logged models that match the specified search criteria.\u001b[39;00m\n\u001b[32m   1000\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1027\u001b[39m \u001b[33;03m    :py:class:`LoggedModel <mlflow.entities.LoggedModel>` objects.\u001b[39;00m\n\u001b[32m   1028\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1029\u001b[39m req_body = message_to_json(\n\u001b[32m   1030\u001b[39m     SearchLoggedModels(\n\u001b[32m   1031\u001b[39m         experiment_ids=experiment_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1051\u001b[39m     )\n\u001b[32m   1052\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m response_proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_endpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSearchLoggedModels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreq_body\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m models = [LoggedModel.from_proto(x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m response_proto.models]\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m PagedList(models, response_proto.next_page_token \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\store\\tracking\\rest_store.py:203\u001b[39m, in \u001b[36mRestStore._call_endpoint\u001b[39m\u001b[34m(self, api, json_body, endpoint, retry_timeout_seconds, response_proto)\u001b[39m\n\u001b[32m    201\u001b[39m     endpoint, method = \u001b[38;5;28mself\u001b[39m._METHOD_TO_INFO[api]\n\u001b[32m    202\u001b[39m response_proto = response_proto \u001b[38;5;129;01mor\u001b[39;00m api.Response()\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_endpoint\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_host_creds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mjson_body\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_proto\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    209\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretry_timeout_seconds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    210\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:596\u001b[39m, in \u001b[36mcall_endpoint\u001b[39m\u001b[34m(host_creds, endpoint, method, json_body, response_proto, extra_headers, retry_timeout_seconds)\u001b[39m\n\u001b[32m    593\u001b[39m     call_kwargs[\u001b[33m\"\u001b[39m\u001b[33mjson\u001b[39m\u001b[33m\"\u001b[39m] = json_body\n\u001b[32m    594\u001b[39m     response = http_request(**call_kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m596\u001b[39m response = \u001b[43mverify_rest_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    597\u001b[39m response_to_parse = response.text\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\mlflow\\utils\\rest_utils.py:315\u001b[39m, in \u001b[36mverify_rest_response\u001b[39m\u001b[34m(response, endpoint)\u001b[39m\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response.status_code != \u001b[32m200\u001b[39m:\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _can_parse_as_json_object(response.text):\n\u001b[32m--> \u001b[39m\u001b[32m315\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m RestException(json.loads(response.text))\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    317\u001b[39m         base_msg = (\n\u001b[32m    318\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAPI request to endpoint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mendpoint\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    319\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mfailed with error code \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m != 200\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    320\u001b[39m         )\n",
      "\u001b[31mRestException\u001b[39m: INTERNAL_ERROR: Response: {'error': 'unsupported endpoint, please contact support@dagshub.com'}"
     ]
    }
   ],
   "source": [
    "#result = dagshub.commit.commit_all(repo_owner='mostafaehabakl', repo_name='DEPI_Graduation_Project', commit_message='Logged all models and their parameters and metrics to MLflow via DagsHub integration.')\n",
    "model_name = \"DEPI_CNN+BiGRU_Model\"\n",
    "model_run_id = \"818297c5bb784d47adc6affb72e5e5be\"\n",
    "model_uri = f\"runs:/{model_run_id}/model\"\n",
    "result = mlflow.register_model(model_uri, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client = mlflow.tracking.MlflowClient()\n",
    "# latest_version_info = client.get_latest_versions(name=model_name, stages=[\"None\"])\n",
    "\n",
    "client = mlflow.MlflowClient()\n",
    "client.copy_model_version(src_model_uri=model_uri, dst_model_name=\"Final_DEPI_Model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# -------------------------------\n",
    "# 1- Generate history.csv (30 days)\n",
    "# -------------------------------\n",
    "num_days = 30\n",
    "start_date = datetime.today() - timedelta(days=num_days)\n",
    "\n",
    "dates = [start_date + timedelta(days=i) for i in range(num_days)]\n",
    "actual_sales = np.random.randint(100, 200, size=num_days)          # random actual sales\n",
    "predictions = actual_sales + np.random.randint(-15, 15, size=num_days)  # random predictions\n",
    "errors = predictions - actual_sales\n",
    "rmse = np.sqrt(errors**2)\n",
    "mape = np.abs(errors / actual_sales) * 100\n",
    "mae = np.abs(errors)\n",
    "drift_flag = (mape > 15).astype(int)  # drift if MAPE > 15%\n",
    "\n",
    "history_df = pd.DataFrame({\n",
    "    \"date\": [d.strftime(\"%Y-%m-%d\") for d in dates],\n",
    "    \"prediction\": predictions,\n",
    "    \"actual\": actual_sales,\n",
    "    \"rmse\": rmse,\n",
    "    \"mape\": mape,\n",
    "    \"mae\": mae,\n",
    "    \"drift_flag\": drift_flag\n",
    "})\n",
    "\n",
    "history_df.to_csv(\"monitoring/history.csv\", index=False)\n",
    "print(\" monitoring/history.csv created.\")\n",
    "\n",
    "# -------------------------------\n",
    "# 2- Generate today_events.csv (1 day)\n",
    "# -------------------------------\n",
    "today_date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "today_actual = np.random.randint(100, 200, size=1)\n",
    "\n",
    "today_df = pd.DataFrame({\n",
    "    \"date\": [today_date],\n",
    "    \"actual_value\": today_actual\n",
    "})\n",
    "\n",
    "today_df.to_csv(\"incoming_data/today_events.csv\", index=False)\n",
    "print(\"‚úÖ incoming_data/today_events.csv created.\")\n",
    "\n",
    "# Optional: preview the files\n",
    "print(history_df.head())\n",
    "print(today_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For static test view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created fake incoming_data/today_events.csv\n",
      "Created fake monitoring/history.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from datetime import datetime, timedelta\n",
    "\n",
    "# # Create folders if they don't exist\n",
    "# os.makedirs(\"incoming_data\", exist_ok=True)\n",
    "# os.makedirs(\"monitoring\", exist_ok=True)\n",
    "\n",
    "# # -------------------------------\n",
    "# # 1. Fake today_events.csv\n",
    "# # -------------------------------\n",
    "# num_today = 30\n",
    "# today_dates = pd.date_range(start=datetime.today(), periods=num_today).strftime(\"%Y-%m-%d\")\n",
    "# today_values = np.random.randint(1000, 5000, size=num_today)  # fake sales\n",
    "\n",
    "# today_df = pd.DataFrame({\n",
    "#     \"date\": today_dates,\n",
    "#     \"actual_value\": today_values\n",
    "# })\n",
    "\n",
    "# today_df.to_csv(\"incoming_data/today_events.csv\", index=False)\n",
    "# print(\"Created fake incoming_data/today_events.csv\")\n",
    "\n",
    "# # -------------------------------\n",
    "# # 2. Fake history.csv\n",
    "# # -------------------------------\n",
    "# num_history = 60\n",
    "# history_dates = pd.date_range(start=datetime.today() - timedelta(days=num_history), periods=num_history).strftime(\"%Y-%m-%d\")\n",
    "# history_actuals = np.random.randint(1000, 5000, size=num_history)\n",
    "# history_preds = history_actuals + np.random.randint(-500, 500, size=num_history)  # small noise\n",
    "\n",
    "# history_df = pd.DataFrame({\n",
    "#     \"date\": history_dates,\n",
    "#     \"truth\": history_actuals,\n",
    "#     \"pred\": history_preds\n",
    "# })\n",
    "\n",
    "# history_df.to_csv(\"monitoring/history.csv\", index=False)\n",
    "# print(\"Created fake monitoring/history.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7863\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\gradio\\queueing.py\", line 763, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\gradio\\route_utils.py\", line 354, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\gradio\\blocks.py\", line 2106, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\gradio\\blocks.py\", line 1588, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(  # type: ignore\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2476, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\gradio\\utils.py\", line 1048, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\MOSTAFA\\AppData\\Local\\Temp\\ipykernel_20184\\1256630244.py\", line 101, in monitor\n",
      "    new_rows = pd.DataFrame({\n",
      "               ^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\pandas\\core\\frame.py\", line 778, in __init__\n",
      "    mgr = dict_to_mgr(data, index, columns, dtype=dtype, copy=copy, typ=manager)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 503, in dict_to_mgr\n",
      "    return arrays_to_mgr(arrays, columns, index, dtype=dtype, typ=typ, consolidate=copy)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 114, in arrays_to_mgr\n",
      "    index = _extract_index(arrays)\n",
      "            ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\DEPI\\Round#3\\Code\\venv\\Lib\\site-packages\\pandas\\core\\internals\\construction.py\", line 664, in _extract_index\n",
      "    raise ValueError(\"Per-column arrays must each be 1-dimensional\")\n",
      "ValueError: Per-column arrays must each be 1-dimensional\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import gradio as gr\n",
    "from scipy.stats import ks_2samp\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 1. Load OLD model (previous version)\n",
    "# -----------------------------------------------------------------------\n",
    "# mlflow.set_tracking_uri(\"https://dagshub.com/YOUR_USERNAME/YOUR_REPO.mlflow\")\n",
    "\n",
    "# model_name = \"DEPI_CNN+BiGRU_Model\"\n",
    "# latest_model = mlflow.pyfunc.load_model(f\"models:/{model_name}/latest\")\n",
    "\n",
    "latest_model = load_model(\"models/cnn_bigru_model.keras\")\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 2. Load TODAY‚Äôs new batch of data\n",
    "# Replace with your actual pipeline\n",
    "# -----------------------------------------------------------------------\n",
    "def load_new_batch():\n",
    "    df = pd.read_csv(\"incoming_data/today_events.csv\")  \n",
    "    return df\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 3. Performance Computation\n",
    "# -----------------------------------------------------------------------\n",
    "def compute_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(np.mean((y_true - y_pred) ** 2))\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-9))) * 100\n",
    "\n",
    "    return rmse, mae, mape\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 4. DRIFT DETECTION FUNCTIONS\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "# Kolmogorov‚ÄìSmirnov test for covariate drift\n",
    "def detect_covariate_drift(old_data, new_data, feature):\n",
    "    stat, pvalue = ks_2samp(old_data[feature], new_data[feature])\n",
    "    drift = pvalue < 0.05\n",
    "    return drift, pvalue\n",
    "\n",
    "\n",
    "# Drift in predictions distribution\n",
    "def detect_prediction_drift(old_preds, new_preds):\n",
    "    stat, pvalue = ks_2samp(old_preds, new_preds)\n",
    "    drift = pvalue < 0.05\n",
    "    return drift, pvalue\n",
    "\n",
    "\n",
    "# Error drift (model degradation)\n",
    "def detect_error_drift(old_errors, new_errors):\n",
    "    stat, pvalue = ks_2samp(old_errors, new_errors)\n",
    "    drift = pvalue < 0.05\n",
    "    return drift, pvalue\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 5. MAIN MONITORING FUNCTION\n",
    "# -----------------------------------------------------------------------\n",
    "\n",
    "def monitor():\n",
    "    # Load today's batch\n",
    "    new_df = load_new_batch()\n",
    "\n",
    "    # Extract X, y\n",
    "    # X_new = np.vstack(new_df[\"date\"].values)\n",
    "    # y_new = new_df[\"actual_value\"].values\n",
    "\n",
    "    SEQ_LEN = 30\n",
    "    X_new = np.random.rand(len(new_df), SEQ_LEN, 1)\n",
    "    y_new = new_df[\"actual_value\"].values\n",
    "\n",
    "    # Predict using current model\n",
    "    y_pred = latest_model.predict(X_new)\n",
    "\n",
    "    # Load previous performance history\n",
    "    history_path = \"monitoring/history.csv\"\n",
    "    try:\n",
    "        hist = pd.read_csv(history_path)\n",
    "        old_preds = hist[\"prediction\"].values\n",
    "        old_truth = hist[\"actual\"].values\n",
    "        old_errors = np.abs(old_truth - old_preds)\n",
    "    except:\n",
    "        hist = pd.DataFrame(columns=[\"date\", \"actual\", \"prediction\"])\n",
    "        old_preds = old_truth = old_errors = np.array([])\n",
    "\n",
    "    # Compute today‚Äôs performance\n",
    "    rmse, mae, mape = compute_metrics(y_new, y_pred)\n",
    "\n",
    "    # Save new performance\n",
    "    today = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    new_rows = pd.DataFrame({\n",
    "        \"date\": [today] * len(y_new),\n",
    "        \"truth\": y_new,\n",
    "        \"pred\": y_pred,\n",
    "    })\n",
    "    hist = pd.concat([hist, new_rows], ignore_index=True)\n",
    "    hist.to_csv(history_path, index=False)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # DRIFT DETECTION\n",
    "    # --------------------------------------------------\n",
    "    drift_results = {}\n",
    "\n",
    "    # 1. Covariate drift\n",
    "    feature = \"actual_value\"  # Example feature\n",
    "    drift_cov, p_cov = detect_covariate_drift(\n",
    "        old_data=new_df, new_data=new_df, feature=feature\n",
    "    )\n",
    "    drift_results[\"Covariate Drift (actual_value)\"] = (drift_cov, p_cov)\n",
    "    # 2. Prediction drift\n",
    "    if len(old_preds) > 50:\n",
    "        drift_pred, p_pred = detect_prediction_drift(old_preds, y_pred)\n",
    "        drift_results[\"Prediction Drift\"] = (drift_pred, p_pred)\n",
    "    else:\n",
    "        drift_results[\"Prediction Drift\"] = (\"Not enough data\", \"-\")\n",
    "\n",
    "    # 3. Error drift\n",
    "    new_errors = np.abs(y_new - y_pred)\n",
    "    if len(old_errors) > 50:\n",
    "        drift_err, p_err = detect_error_drift(old_errors, new_errors)\n",
    "        drift_results[\"Error Drift\"] = (drift_err, p_err)\n",
    "    else:\n",
    "        drift_results[\"Error Drift\"] = (\"Not enough data\", \"-\")\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # MLflow automatic logging\n",
    "    # --------------------------------------------------\n",
    "    with mlflow.start_run(run_name=f\"monitoring_{today}\"):\n",
    "        mlflow.log_metric(\"rmse\", rmse)\n",
    "        mlflow.log_metric(\"mae\", mae)\n",
    "        mlflow.log_metric(\"mape\", mape)\n",
    "\n",
    "    # --------------------------------------------------\n",
    "    # Visualization\n",
    "    # --------------------------------------------------\n",
    "    fig = px.line(hist, x=\"date\", y=\"actual\", title=\"Actual vs Predicted (Truth)\")\n",
    "    \n",
    "    drift_alerts = \"\\n\".join(\n",
    "        f\"{k}: {'‚ö†Ô∏è DRIFT' if v[0] == True else 'OK'} (p={v[1]})\"\n",
    "        for k, v in drift_results.items()\n",
    "    )\n",
    "\n",
    "    return (\n",
    "        f\"RMSE: {rmse:.4f}\\nMAE: {mae:.4f}\\nMAPE: {mape:.2f}%\\n\\n\"\n",
    "        + \"--- DRIFT REPORT ---\\n\"\n",
    "        + drift_alerts,\n",
    "        fig\n",
    "    )\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------\n",
    "# 6. GRADIO DASHBOARD\n",
    "# -----------------------------------------------------------------------\n",
    "with gr.Blocks(title=\"Model Monitoring & Drift Detection\") as dashboard:\n",
    "    gr.Markdown(\"# üìä Model Monitoring & Drift Detection Dashboard\")\n",
    "    gr.Markdown(\"Automatically monitors performance, drift, and MLflow logs.\")\n",
    "\n",
    "    output_text = gr.Textbox(label=\"Performance & Drift Report\", lines=10)\n",
    "    output_plot = gr.Plot(label=\"Performance History\")\n",
    "\n",
    "    run_button = gr.Button(\"Run Monitoring Now\")\n",
    "    run_button.click(monitor, outputs=[output_text, output_plot])\n",
    "\n",
    "dashboard.launch(server_name=\"0.0.0.0\", server_port=7863 , share=False)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 426888,
     "sourceId": 1015349,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv (3.12.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
